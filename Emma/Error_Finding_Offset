import numpy as np
import pandas as pd
import plotly.graph_objects as go
import seaborn as sns
import os
import webbrowser
from tempfile import NamedTemporaryFile
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt

raw_data = pd.read_csv('Sample data/21006846/VST_RAW.txt', sep = ';', decimal = ',', skiprows = 3, names = ['Date', 'Value'], encoding = 'latin-1')
editted_level_data = pd.read_csv('Sample data/21006846/VST_EDT.txt', sep = ';', decimal = ',', skiprows = 3, names = ['Date', 'Value'], encoding = 'latin-1')
vinge_data = pd.read_excel('Sample data/21006846/VINGE.xlsm', decimal = ',', header = 0)
#precipitation_data = pd.read_csv('/Users/emmamork/Library/CloudStorage/OneDrive-DanmarksTekniskeUniversitet/Master Thesis/MasterThesis/Emma/Sample data/RainData_05205.csv', parse_dates=['datetime'])

raw_data['Value'] = raw_data['Value'].astype(float)
# Convert Date column to datetime with specified format (YYYY-MM-DD HH:MM:SS)
raw_data['Date'] = pd.to_datetime(raw_data['Date'], format='%Y-%m-%d %H:%M:%S')
editted_level_data['Date'] = pd.to_datetime(editted_level_data['Date'], format='%d-%m-%Y %H:%M')
vinge_data['Date'] = pd.to_datetime(vinge_data['Date'], format='%d.%m.%Y %H:%M')

# Crop all dataframes to start from 2010-01-01
start_date = '2000-01-01'
raw_data = raw_data[raw_data['Date'] >= start_date]
editted_level_data = editted_level_data[editted_level_data['Date'] >= start_date]
vinge_data = vinge_data[vinge_data['Date'] >= start_date]
#precipitation_data = precipitation_data[precipitation_data['datetime'] >= start_date]

# Multiply W.L [cm] by 100 to convert to mm
vinge_data['W.L [cm]'] = vinge_data['W.L [cm]']*10

def detect_offset_errors(df, threshold=100, window_size=24, min_duration_hours=1):
    """
    Detect offset errors by identifying sustained level shifts that later return to the original level
    """
    diffs = df['Value'].diff().values
    dates = df['Date'].values
    values = df['Value'].values
    time_diffs = df['Date'].diff().values
    
    # Convert 15 minutes to nanoseconds
    fifteen_min = np.timedelta64(20, 'm')  # Slightly increased time window
    
    # Add detailed debugging for the specific period
    sept_11 = pd.Timestamp('2008-09-11')
    oct_1 = pd.Timestamp('2008-10-01')
    mask = (df['Date'] >= sept_11) & (df['Date'] <= oct_1)
    period_data = df[mask]
    
    print("\nAnalyzing September 11 - October 1 period:")
    print(f"Number of measurements in period: {len(period_data)}")
    print(f"Max diff in period: {period_data['Value'].diff().abs().max()}")
    print(f"Mean value before jump: {df[df['Date'] < sept_11]['Value'].tail(24).mean():.2f}")
    print(f"Mean value during offset: {period_data['Value'].mean():.2f}")
    print(f"Time differences in period: {period_data['Date'].diff().value_counts().head()}")
    
    # Print the exact values around September 11
    sept_11_data = df[(df['Date'] >= sept_11 - pd.Timedelta(days=1)) & 
                      (df['Date'] <= sept_11 + pd.Timedelta(days=1))]
    print("\nValues around September 11:")
    for _, row in sept_11_data.iterrows():
        print(f"Date: {row['Date']}, Value: {row['Value']:.2f}")
    
    offset_periods = []
    i = window_size
    
    while i < len(df) - window_size:
        current_date = dates[i]
        current_diff = diffs[i]
        current_time_diff = time_diffs[i]
        
        # Print info for large jumps
        if abs(current_diff) > threshold:
            print(f"\nPotential jump found:")
            print(f"Date: {current_date}")
            print(f"Jump size: {current_diff:.2f}")
            print(f"Time difference: {current_time_diff}")
        
        # Check for sudden jump with 15-min measurement interval
        if abs(current_diff) > threshold and current_time_diff <= fifteen_min:
            jump_value = current_diff
            start_date = current_date
            base_level = values[i-1]
            
            # Check if the new level is maintained
            future_values = values[i:i+window_size]
            level_stability = np.std(future_values[1:min(24, len(future_values))])
            
            # More lenient stability check
            if level_stability < abs(jump_value) * 0.3:  # Increased from 0.2 to 0.3
                # Look for return to original level
                for j in range(i + 24, min(i + window_size * 24, len(df))):
                    return_jump = values[j] - values[j-1]
                    
                    # More lenient return jump check
                    if (abs(return_jump + jump_value) < abs(jump_value) * 0.3 and  # Increased from 0.2 to 0.3
                        time_diffs[j] <= fifteen_min):
                        
                        duration = pd.Timedelta(dates[j] - start_date)
                        
                        if duration.total_seconds() / 3600 >= min_duration_hours:
                            offset_periods.append({
                                'start_date': start_date,
                                'end_date': dates[j],
                                'jump_size': jump_value,
                                'duration_hours': duration.total_seconds() / 3600
                            })
                            i = j  # Skip to end of offset period
                            break
            
        i += 1
    
    return pd.DataFrame(offset_periods)

# Detect offset errors using optimized function
offset_errors = detect_offset_errors(raw_data)

# Create an interactive plot using plotly - optimize for performance
fig = go.Figure()

# Downsample data for plotting if dataset is large
def downsample_data(df, target_points=3000):
    if len(df) > target_points:
        step = len(df) // target_points
        return df.iloc[::step]
    return df

# Downsample datasets for plotting
raw_data_plot = downsample_data(raw_data)
editted_data_plot = downsample_data(editted_level_data)

# Add traces for each dataset
fig.add_trace(
    go.Scatter(
        x=raw_data['Date'],
        y=raw_data['Value'],
        name='Raw Data',
        mode='lines'
    )
)

fig.add_trace(
    go.Scatter(
        x=editted_level_data['Date'],
        y=editted_level_data['Value'],
        name='Edited Level Data',
        mode='lines'
    )
)

fig.add_trace(
    go.Scatter(
        x=vinge_data['Date'],
        y=vinge_data['W.L [cm]'],
        name='Vinge Data',
        mode='markers',
        marker=dict(size=6)
    )
)

# Batch add all vrects at once for better performance
shapes = [
    dict(
        type="rect",
        xref="x",
        yref="paper",
        x0=error['start_date'],
        x1=error['end_date'],
        y0=0,
        y1=1,
        fillcolor="red",
        opacity=0.2,
        layer="below",
        line_width=0,
    )
    for _, error in offset_errors.iterrows()
]

# Update layout with all shapes at once
fig.update_layout(
    title='Water Level Data Comparison',
    xaxis_title='Date',
    yaxis_title='Water Level (mm)',
    hovermode='x unified',
    template='plotly_white',
    showlegend=True,
    shapes=shapes
)

# Save the plot to a temporary HTML file and open it in the browser
temp_file = NamedTemporaryFile(delete=False, suffix='.html')
fig.write_html(temp_file.name)
webbrowser.open('file://' + temp_file.name)

